# -*- coding: utf-8 -*-
"""PySpark_project_stepik_script

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J1XhXaqj32PyB245a5KnmfNTjQ3VqeuC
"""

import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, datediff
from pyspark.sql import functions as F

def reading(input_path):
  '''Читаем данные из файла'''
  df = spark.read.parquet(input_path)
  return df

def is_cpc_cpm(df):
  '''Добавляем признаки CPC\CPM.
     is_cpm: 1 если тип объявления CPM, иначе 0.
     is_cpc: 1 если тип объявления CPC, иначе 0.
  '''
  df = df.withColumn('is_cpc',F.when(df.ad_cost_type == 'CPC', 1).otherwise(0))
  df = df.withColumn('is_cpm',F.when(df.ad_cost_type == 'CPM', 1).otherwise(0))
  return df

def ctr_adding(df):
  '''Добавление колонки CTR.
    CTR равен отношению числа кликов к числу просмотров.
  '''
  #временно добавим колонки с признаком клика или просмотра
  df = df.withColumn('click',F.when(df.event == 'click', 1).otherwise(0))
  df = df.withColumn('view',F.when(df.event == 'view', 1).otherwise(0))

  #сгруппируем по ad_id, просуммируем клики и просмотры для дальнейшего расчета CTR
  gdf = df.groupby('ad_id')\
  .agg({'click':'sum','view':'sum'})\
  .withColumnRenamed('sum(click)','clicks')\
  .withColumnRenamed('sum(view)', 'views')

  #рассчитаем ctr
  gdf = gdf.withColumn('ctr', F.round(gdf.clicks/gdf.views,3)*100)

  #добавим ctr в таблицу
  df = df.join(gdf, on='ad_id')
  return df  

def day_count_adding(df):
  '''Добавляем колонку day_count.
    Это число дней, которое показывалась реклама.
  '''
  #находим разность между текущей датой и датой объявления
  df = df.withColumn('day_count', datediff(F.current_date(),df.date))
  return df

def cols_deleting(df):
  '''Удаляем ненужные колонки из таблицы'''
  cols = ['ad_cost_type','event','client_union_id','platform','click','view','views','clicks', 'time', 'date', 'compaign_union_id']
  #выбросим ненужные колонки
  df = df.drop(*cols)
  return df

def df_splitting(df):
  '''Разделяем данные в соотношении 0.75 к 0.25'''
  train, test = df.randomSplit([0.75,0.25], 17)
  return train, test

def writing_to_file(train, test, output_path):
  '''Записываем наборы данных в отдельные файлы'''
  test.coalesce(1).write.parquet('{}/test'.format(output_path))
  train.coalesce(1).write.parquet('{}/train'.format(output_path))

def process(spark, input_path, output_path):
  '''Основная функция, которая обрабатывает данные
  и записывает их в файлы
  '''
  df = reading(input_path)
  df = is_cpc_cpm(df)
  df = ctr_adding(df)
  df = day_count_adding(df)
  df = cols_deleting(df)
  train, test = df_splitting(df)
  writing_to_file(train, test, output_path)

def main(argv):
  '''Определеяем путь до файла, путь до конечной папки,
  создаем спар сессию
  '''
  input_path = argv[0]
  print("Input path to file: " + input_path)
  target_path = argv[1]
  print("Target path: " + target_path)
  spark = _spark_session()
  process(spark, input_path, target_path)

def _spark_session():
  '''Функция создает спар сессию '''
  return SparkSession.builder.appName('PySparkJob').getOrCreate()

if __name__ == "__main__":
  arg = sys.argv[1:]
  if len(arg) != 2:
    sys.exit("Input and Target path are require.")
  else:
    main(arg)